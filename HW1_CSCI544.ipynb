{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HW 1 Sentiment Analysis - HW Report as Jupyter Notebook pdf Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2XxfPw-fQH1",
        "outputId": "19dd26ec-62b5-483e-b8fd-e28d54c5128b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2023.8.8)\n",
            "Requirement already satisfied: tqdm in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.2)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: contractions in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bs4) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (pyproject.toml): started\n",
            "  Building wheel for bs4 (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1265 sha256=e1fb83e5e4e646f35fd3307128ee2fa82dc925c04c2ffa82f84a495534503e64\n",
            "  Stored in directory: c:\\users\\krusa\\appdata\\local\\pip\\cache\\wheels\\d4\\c8\\5b\\b5be9c20e5e4503d04a6eac8a3cd5c2393505c29f02bea0960\n",
            "Successfully built bs4\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.11.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\krusa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install beautifulsoup4\n",
        "%pip install contractions\n",
        "%pip install bs4\n",
        "%pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code uses Jupyter Notebook magic commands to install several Python packages commonly used in natural language processing and machine learning tasks. These packages include NLTK for text processing, BeautifulSoup4 for web scraping and HTML/XML parsing, Contractions for handling word contractions, and scikit-learn for machine learning tasks. These libraries enable data preprocessing, web data extraction, and machine learning model development for tasks like text analysis and classification. The code ensures that the required packages are installed and ready for use in the Jupyter Notebook environment, facilitating various text-based analyses and report generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a53XlPue8rl",
        "outputId": "db30cd26-bb4f-4428-bcac-e913efec4de9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\krusa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\krusa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\krusa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\krusa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "url= \"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code imports necessary libraries and downloads NLTK resources for text preprocessing. It also sets the `url` variable to a URL pointing to an Amazon reviews dataset in TSV format. The code can be part of a data preprocessing pipeline for text analysis. However, it lacks further processing steps and the actual dataset loading. You would typically use libraries like `pandas` and `requests` or `wget` to download and load the dataset from the URL, followed by additional steps like data cleaning, tokenization, and splitting into training and testing sets using `train_test_split` for a machine learning project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD6gqkdle8ro"
      },
      "source": [
        "## Read Data\n",
        "## Keep Reviews and Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blP-wf0he8rp",
        "outputId": "d2a07a0c-1b50-45ac-8c75-32f0e941ba0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\krusa\\AppData\\Local\\Temp\\ipykernel_29028\\3982505177.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv', sep=r\"\\t+\", usecols=['star_rating', 'review_body'])\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('amazon_reviews_us_Office_Products_v1_00.tsv', sep=r\"\\t+\", usecols=['star_rating', 'review_body'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This line of code reads a TSV file named 'amazon_reviews_us_Office_Products_v1_00.tsv' into a Pandas DataFrame called 'df.' It uses a tab character as the separator and selects only the 'star_rating' and 'review_body' columns from the dataset. This allows for efficient data manipulation and analysis of these specific columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_LJ19bFe8rq"
      },
      "source": [
        " ## We form two classes and select 50000 reviews randomly from each class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['star_rating'] = df['star_rating'].apply(lambda x: 2 if x in [4,5] else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This line of code modifies the 'star_rating' column in the Pandas DataFrame 'df.' It applies a lambda function to each element in the 'star_rating' column. If a value in the 'star_rating' column is either 4 or 5, it is replaced with the value 2; otherwise, it is replaced with the value 1. This operation essentially converts a rating system with 4 and 5 stars to a binary classification where 4 and 5 stars are considered positive (2) and all other ratings are considered negative (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zDUjGu2qe8rq"
      },
      "outputs": [],
      "source": [
        "random_state = 25\n",
        "class1_samples = df[df['star_rating'] == 1].sample(n=50000, random_state=random_state)\n",
        "class2_samples = df[df['star_rating'] == 2].sample(n=50000, random_state=random_state)\n",
        "class1_samples = class1_samples.reset_index(drop=True)\n",
        "class2_samples = class2_samples.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code sets a random seed for reproducibility and selects random samples from two classes (1 and 2) in the DataFrame `df`. It creates two new DataFrames, `class1_samples` and `class2_samples`, each containing 50,000 randomly chosen rows from their respective classes. The `reset_index(drop=True)` statements reset the index of these DataFrames for better organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WY3Mpa1Ae8rq"
      },
      "outputs": [],
      "source": [
        "final_df = pd.concat([class1_samples, class2_samples], ignore_index=True)\n",
        "final_df = final_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code concatenates the two DataFrames, `class1_samples` and `class2_samples`, into a single DataFrame named `final_df`. The `ignore_index=True` argument ensures that the index is reset to maintain a continuous index for the combined DataFrame. The subsequent `reset_index(drop=True)` line further resets the index to maintain continuity. As a result, `final_df` contains the combined data with a consistent index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpIYYzWre8rq"
      },
      "source": [
        "# Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY8Yv41me8rr",
        "outputId": "5aaf2217-6fe2-4b9d-d483-014b43b7add6"
      },
      "outputs": [],
      "source": [
        "flenDC = final_df['review_body'].str.len().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This line of code calculates the mean (average) length of the text in the 'review_body' column of the Pandas DataFrame 'final_df' and assigns it to the variable 'flenDC'. Essentially, it computes the average number of characters in the 'review_body' text for all the rows in the DataFrame, providing insight into the typical length of reviews in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LbDp0Zhje8rr"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text):\n",
        "    if text is None:\n",
        "        return None\n",
        "    return contractions.fix(str(text))\n",
        "\n",
        "final_df['review_body'] = final_df['review_body'].str.lower()\n",
        "\n",
        "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "html_pattern = r'<.*?>'\n",
        "non_alpha_pattern = r'[^a-zA-Z\\s]'\n",
        "\n",
        "final_df['review_body'] = final_df['review_body'].str.replace(url_pattern, '', regex=True)\n",
        "final_df['review_body'] = final_df['review_body'].str.replace(html_pattern, '', regex=True)\n",
        "final_df['review_body'] = final_df['review_body'].str.replace(non_alpha_pattern, '', regex=True)\n",
        "final_df['review_body'] = final_df['review_body'].str.replace(r'\\s+', ' ', regex=True)\n",
        "final_df['review_body'] = final_df['review_body'].apply(expand_contractions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code defines a series of text preprocessing steps applied to the 'review_body' column of the DataFrame 'final_df'. Here's a breakdown of each step:\n",
        "\n",
        "1. `expand_contractions`: This is a custom function that uses the 'contractions' library to expand contractions in text. It is applied to each element in the 'review_body' column to ensure that contractions like \"can't\" are converted to their full forms, such as \"cannot.\"\n",
        "\n",
        "2. `final_df['review_body'] = final_df['review_body'].str.lower()`: This line converts all text in the 'review_body' column to lowercase, making it consistent for further processing.\n",
        "\n",
        "3. `url_pattern = r'https?://\\S+|www\\.\\S+'`: This regular expression pattern (`url_pattern`) is used to identify and remove URLs and website addresses from the text.\n",
        "\n",
        "4. `html_pattern = r'<.*?>'`: This regular expression pattern (`html_pattern`) identifies and removes HTML tags from the text, effectively cleaning any HTML markup.\n",
        "\n",
        "5. `non_alpha_pattern = r'[^a-zA-Z\\s]'`: This regular expression pattern (`non_alpha_pattern`) matches any characters that are not alphabetic letters or whitespace. It is used to remove non-alphabetic characters from the text.\n",
        "\n",
        "6. The next several lines apply the defined regular expression patterns to the 'review_body' column using the `str.replace()` method, effectively removing URLs, HTML tags, non-alphabetic characters, and extra whitespace.\n",
        "\n",
        "After running these preprocessing steps, the 'review_body' column in the 'final_df' DataFrame will be cleaned and ready for further text analysis or machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiYWz4Fre8rr",
        "outputId": "43ee2999-c4cc-4c21-f416-450adb8cc4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "185.14938,185.19607\n"
          ]
        }
      ],
      "source": [
        "slenDC = final_df['review_body'].str.len().mean()\n",
        "print(f\"{flenDC},{slenDC}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypSiigOve8rr"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36kloU-xe8rs"
      },
      "source": [
        "## remove the stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VMUOaIMe8rs"
      },
      "source": [
        "## perform lemmatization  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "flenPP = final_df['review_body'].str.len().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fmnU3lv8e8rs"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if text is None:\n",
        "        return None\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "final_df['review_body'] = final_df['review_body'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code defines a text preprocessing function `preprocess_text` and applies it to the 'review_body' column of the 'final_df' DataFrame. Here's what each part of the code does:\n",
        "\n",
        "1. `lemmatizer = WordNetLemmatizer()`: It initializes a WordNet lemmatizer, which is used for lemmatization, reducing words to their base or dictionary form.\n",
        "\n",
        "2. `stop_words = set(stopwords.words('english'))`: It creates a set of English stopwords using NLTK's built-in stopwords list. These stopwords are words that are commonly removed from text during preprocessing because they typically do not provide meaningful information.\n",
        "\n",
        "3. `def preprocess_text(text)`: This is a custom preprocessing function that takes an input text as an argument.\n",
        "\n",
        "   - It tokenizes the text into individual words using `nltk.word_tokenize(text)`.\n",
        "\n",
        "   - Then, it removes any stopwords from the tokenized words, ensuring that common, non-informative words are excluded.\n",
        "\n",
        "   - Next, it lemmatizes each word using the WordNet lemmatizer, converting words to their base forms.\n",
        "\n",
        "   - Finally, it returns the preprocessed text as a string by joining the lemmatized tokens with spaces.\n",
        "\n",
        "4. `final_df['review_body'] = final_df['review_body'].apply(preprocess_text)`: This line applies the `preprocess_text` function to each element in the 'review_body' column of the 'final_df' DataFrame. It effectively preprocesses the text in the 'review_body' column by tokenizing, removing stopwords, and lemmatizing the words.\n",
        "\n",
        "After executing this code, the 'review_body' column in 'final_df' will contain preprocessed text data ready for use in natural language processing or machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "185.19607,185.04318\n"
          ]
        }
      ],
      "source": [
        "slenPP = final_df['review_body'].str.len().mean()\n",
        "print(f\"{flenPP},{slenPP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Data into two parts (train_df and test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pQzQ5bv1e8rs"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(final_df, test_size=0.2, random_state=random_state)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_labels = train_df['star_rating']\n",
        "test_labels = test_df['star_rating']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code performs data splitting for machine learning tasks. It begins by using scikit-learn's train_test_split function to divide the 'final_df' DataFrame into two sets: 'train_df' (80% of the data) and 'test_df' (20% of the data). The test_size parameter controls the portion allocated for testing, while random_state ensures the process is reproducible.\n",
        "\n",
        "Subsequently, both 'train_df' and 'test_df' have their indices reset to create a continuous and consistent index structure.\n",
        "\n",
        "Finally, the code extracts the 'star_rating' column from both 'train_df' and 'test_df' and assigns them to 'train_labels' and 'test_labels,' respectively. These labels are essential for supervised machine learning, where the goal is often to predict or classify based on the provided labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbPJXOEe8rs"
      },
      "source": [
        "# TF-IDF and BoW Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tGSlVYBLe8rs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  \n",
        "\n",
        "tfidf_train_features = tfidf_vectorizer.fit_transform(train_df['review_body'])\n",
        "tfidf_test_features = tfidf_vectorizer.transform(test_df['review_body'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code utilizes scikit-learn's `TfidfVectorizer` to convert text data into TF-IDF features. It limits the number of features to a maximum of 10,000. The code transforms the text in the 'review_body' column of the training dataset ('train_df') into TF-IDF features stored in 'tfidf_train_features'. Similarly, it transforms the text in the testing dataset ('test_df') into TF-IDF features stored in 'tfidf_test_features'. TF-IDF quantifies word importance in text, aiding machine learning tasks, and the 10,000-feature limit helps manage dimensionality for efficient modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow_vectorizer = CountVectorizer(max_features=10000)\n",
        "\n",
        "bow_train_features = bow_vectorizer.fit_transform(train_df['review_body'])\n",
        "bow_test_features = bow_vectorizer.transform(test_df['review_body'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code imports `CountVectorizer`, initializes it with a maximum of 10,000 features, and then applies it to the 'review_body' column of both the training ('train_df') and testing ('test_df') datasets. This vectorizer transforms the text data into numerical representations based on word frequency. 'bow_train_features' and 'bow_test_features' store the resulting BoW representations for the training and testing data, respectively. BoW represents each document as a vector of word frequencies, making it suitable for various text analysis tasks, including machine learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9rwCewae8rs"
      },
      "source": [
        "# Perceptron Using Both Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Perceptron for TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9noV5wSse8rt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7756 0.8279 0.8009\n"
          ]
        }
      ],
      "source": [
        "perceptron_tfidf = Perceptron()\n",
        "\n",
        "perceptron_tfidf.fit(tfidf_train_features, train_labels)\n",
        "\n",
        "tfidf_predictions = perceptron_tfidf.predict(tfidf_test_features)\n",
        "\n",
        "precision_tfidf = precision_score(test_labels, tfidf_predictions)\n",
        "recall_tfidf = recall_score(test_labels, tfidf_predictions)\n",
        "f1_tfidf = f1_score(test_labels, tfidf_predictions)\n",
        "\n",
        "print(f'{precision_tfidf:.4f} {recall_tfidf:.4f} {f1_tfidf:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. `perceptron_tfidf = Perceptron()`: It initializes a Perceptron classifier.\n",
        "\n",
        "2. `perceptron_tfidf.fit(tfidf_train_features, train_labels)`: The Perceptron is trained using TF-IDF features (`tfidf_train_features`) and corresponding training labels (`train_labels`).\n",
        "\n",
        "3. `tfidf_predictions = perceptron_tfidf.predict(tfidf_test_features)`: The trained Perceptron predicts labels for the testing dataset (`tfidf_test_features`), storing the results in `tfidf_predictions`.\n",
        "\n",
        "4. Precision, recall, and F1-score are computed using scikit-learn's `precision_score`, `recall_score`, and `f1_score` functions.\n",
        "\n",
        "5. The calculated precision, recall, and F1-score are printed, representing the classifier's performance on the test data. These metrics help evaluate the model's accuracy, completeness, and overall effectiveness in classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Perceptron for BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7996 0.7862 0.7928\n"
          ]
        }
      ],
      "source": [
        "perceptron_bow = Perceptron()\n",
        "\n",
        "perceptron_bow.fit(bow_train_features, train_labels)\n",
        "\n",
        "bow_predictions = perceptron_bow.predict(bow_test_features)\n",
        "\n",
        "precision_bow = precision_score(test_labels, bow_predictions)\n",
        "recall_bow = recall_score(test_labels, bow_predictions)\n",
        "f1_bow = f1_score(test_labels, bow_predictions)\n",
        "\n",
        "print(f'{precision_bow:.4f} {recall_bow:.4f} {f1_bow:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSniwHyxe8rt"
      },
      "source": [
        "# SVM Using Both Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. SVM for TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8402 0.8455 0.8428\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "svm_tfidf = LinearSVC(dual=True, random_state=random_state)\n",
        "\n",
        "svm_tfidf.fit(tfidf_train_features, train_labels)\n",
        "\n",
        "svm_tfidf_predictions = svm_tfidf.predict(tfidf_test_features)\n",
        "\n",
        "precision_tfidf = precision_score(test_labels, svm_tfidf_predictions)\n",
        "recall_tfidf = recall_score(test_labels, svm_tfidf_predictions)\n",
        "f1_tfidf = f1_score(test_labels, svm_tfidf_predictions)\n",
        "\n",
        "print(f'{precision_tfidf:.4f} {recall_tfidf:.4f} {f1_tfidf:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. `from sklearn.svm import LinearSVC`: Import the LinearSVC classifier from scikit-learn.\n",
        "\n",
        "2. `from sklearn.metrics import precision_score, recall_score, f1_score`: Import functions for precision, recall, and F1-score calculations.\n",
        "\n",
        "3. `svm_tfidf = LinearSVC(dual=True, random_state=random_state)`: Initialize a LinearSVC classifier with specified options, including `dual=True` and a random seed for reproducibility.\n",
        "\n",
        "4. `svm_tfidf.fit(tfidf_train_features, train_labels)`: Train the LinearSVC classifier using the TF-IDF features and training labels.\n",
        "\n",
        "5. `svm_tfidf_predictions = svm_tfidf.predict(tfidf_test_features)`: Predict labels for the test dataset using the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. SVM for BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\krusa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8404 0.8197 0.8299\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "svm_bow = LinearSVC(max_iter=10000, random_state=42)\n",
        "\n",
        "svm_bow.fit(bow_train_features, train_labels)\n",
        "\n",
        "svm_bow_predictions = svm_bow.predict(bow_test_features)\n",
        "\n",
        "precision_bow = precision_score(test_labels, svm_bow_predictions)\n",
        "recall_bow = recall_score(test_labels, svm_bow_predictions)\n",
        "f1_bow = f1_score(test_labels, svm_bow_predictions)\n",
        "\n",
        "print(f'{precision_bow:.4f} {recall_bow:.4f} {f1_bow:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2P3j7She8rt"
      },
      "source": [
        "# Logistic Regression Using Both Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. LR - TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "mXUNUIJCe8rt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8421 0.8573 0.8497\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "logistic_regression_tfidf = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "logistic_regression_tfidf.fit(tfidf_train_features, train_labels)\n",
        "\n",
        "logistic_regression_tfidf_predictions = logistic_regression_tfidf.predict(tfidf_test_features)\n",
        "\n",
        "precision_tfidf = precision_score(test_labels, logistic_regression_tfidf_predictions)\n",
        "recall_tfidf = recall_score(test_labels, logistic_regression_tfidf_predictions)\n",
        "f1_tfidf = f1_score(test_labels, logistic_regression_tfidf_predictions)\n",
        "\n",
        "print(f'{precision_tfidf:.4f} {recall_tfidf:.4f} {f1_tfidf:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. `from sklearn.linear_model import LogisticRegression`: Import the LogisticRegression classifier from scikit-learn.\n",
        "\n",
        "2. `from sklearn.metrics import precision_score, recall_score, f1_score`: Import functions for precision, recall, and F1-score calculations.\n",
        "\n",
        "3. `logistic_regression_tfidf = LogisticRegression(max_iter=10000, random_state=42)`: Initialize a Logistic Regression classifier with specified options, including a high maximum number of iterations (`max_iter=10000`) and a random seed for reproducibility.\n",
        "\n",
        "4. `logistic_regression_tfidf.fit(tfidf_train_features, train_labels)`: Train the Logistic Regression classifier using the TF-IDF features and training labels.\n",
        "\n",
        "5. `logistic_regression_tfidf_predictions = logistic_regression_tfidf.predict(tfidf_test_features)`: Predict labels for the test dataset using the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. LR - BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8488 0.8320 0.8403\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "logistic_regression_bow = LogisticRegression(max_iter=10000, random_state=42)\n",
        "\n",
        "logistic_regression_bow.fit(bow_train_features, train_labels)\n",
        "\n",
        "logistic_regression_bow_predictions = logistic_regression_bow.predict(bow_test_features)\n",
        "\n",
        "precision_bow = precision_score(test_labels, logistic_regression_bow_predictions)\n",
        "recall_bow = recall_score(test_labels, logistic_regression_bow_predictions)\n",
        "f1_bow = f1_score(test_labels, logistic_regression_bow_predictions)\n",
        "\n",
        "print(f'{precision_bow:.4f} {recall_bow:.4f} {f1_bow:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atV_CKgne8rt"
      },
      "source": [
        "# Naive Bayes Using Both Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Naive Bayes Tf-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8289 0.8111 0.8199\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "naive_bayes_tfidf = MultinomialNB()\n",
        "\n",
        "naive_bayes_tfidf.fit(tfidf_train_features, train_labels)\n",
        "\n",
        "naive_bayes_tfidf_predictions = naive_bayes_tfidf.predict(tfidf_test_features)\n",
        "\n",
        "precision_tfidf = precision_score(test_labels, naive_bayes_tfidf_predictions)\n",
        "recall_tfidf = recall_score(test_labels, naive_bayes_tfidf_predictions)\n",
        "f1_tfidf = f1_score(test_labels, naive_bayes_tfidf_predictions)\n",
        "\n",
        "print(f'{precision_tfidf:.4f} {recall_tfidf:.4f} {f1_tfidf:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. `from sklearn.naive_bayes import MultinomialNB`: Import the Multinomial Naive Bayes classifier from scikit-learn.\n",
        "\n",
        "2. `from sklearn.metrics import precision_score, recall_score, f1_score`: Import functions for precision, recall, and F1-score calculations.\n",
        "\n",
        "3. `naive_bayes_tfidf = MultinomialNB()`: Initialize a Multinomial Naive Bayes classifier.\n",
        "\n",
        "4. `naive_bayes_tfidf.fit(tfidf_train_features, train_labels)`: Train the Multinomial Naive Bayes classifier using the TF-IDF features and training labels.\n",
        "\n",
        "5. `naive_bayes_tfidf_predictions = naive_bayes_tfidf.predict(tfidf_test_features)`: Predict labels for the test dataset using the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Naive Bayes BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "boXnk_rue8rt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8455 0.7489 0.7943\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "naive_bayes_bow = MultinomialNB()\n",
        "\n",
        "naive_bayes_bow.fit(bow_train_features, train_labels) \n",
        "\n",
        "naive_bayes_bow_predictions = naive_bayes_bow.predict(bow_test_features)  \n",
        "\n",
        "precision_bow = precision_score(test_labels, naive_bayes_bow_predictions)\n",
        "recall_bow = recall_score(test_labels, naive_bayes_bow_predictions)\n",
        "f1_bow = f1_score(test_labels, naive_bayes_bow_predictions)\n",
        "\n",
        "print(f'{precision_bow:.4f} {recall_bow:.4f} {f1_bow:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
